{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение в Spark и Python\n",
    "\n",
    "Давайте научимся использовать Spark с Python с помощью библиотеки pyspark! Перед продолжением работы с этим кодом обязательно просмотрите лекцию, объясняющую Spark и RDD.\n",
    "\n",
    "Эта тетрадка будет служить справочным кодом для раздела Big Data (большие данные) курса, влючающие в себя веб-сервисы Amazon. Лекция предоставит более полное объяснение того, что делает код.\n",
    "\n",
    "## Создание SparkContext\n",
    "\n",
    "Сначала нам нужно создать SparkContext. Мы импортируем его из pyspark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь создайте SparkContext. SparkContext представляет собой соединение с кластером Spark и может использоваться для создания RDD и трансляции переменных на этот кластер.\n",
    "\n",
    "*Примечание! У вас будет только один SparkContext за один раз, при нашем рабочем раскладе.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Базовые операции\n",
    "\n",
    "Начнем с примера 'hello world' (привет мир), который просто читает текстовый файл. Для начала создадим текстовый файл."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте напишем пример текстового файла для чтения. Мы используем для этого специальные команды jupyter notebook, но если хотите можете использовать любой .txt файл:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting example.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile example.txt\n",
    "first line\n",
    "second line\n",
    "third line\n",
    "fourth line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем принять текстовый файл, используя метод **textFile** из созданного нами SparkContext. Этот метод прочитает текстовый файл из HDFS, локальной файловой системы (доступной на всех\n",
    "узлах) или из любой URI файловых систем, поддерживаемых Hadoop, и вернет его как строки RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textFile = sc.textFile('example.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основная абстракция Spark - это распределенная коллекция элементов, называемая Resilient Distributed Dataset (RDD) (устойчивый распределенный набор данных). RDD могут быть созданы из Hadoop InputFormats (например, файлов HDFS) или путем преобразования других RDD.\n",
    "\n",
    "### Действия\n",
    "\n",
    "Мы только что создали RDD, используя метод textFile, и можем выполнять операции с этим объектом, такие как подсчет строк.\n",
    "\n",
    "У RDD есть действия, которые возвращают значения, и преобразования, которые возвращают указатели на новые RDD. Давайте начнем с нескольких действий:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'first line'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Преобразования\n",
    "\n",
    "Теперь мы можем использовать преобразования. Например, преобразование фильтра вернет новый RDD с подмножеством элементов в файле. Давайте создадим пример преобразования, используя метод filter(). Этот метод (как и собственная функция фильтра Python) будет возвращать только те элементы, которые удовлетворяют условию. Попробуем поискать строки, содержащие слово «second». В результате чего мы обнаружим, что должна быть только одна такая строка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "secfind = textFile.filter(lambda line: 'second' in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[7] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD\n",
    "secfind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['second line']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Исполните действие на преобразование\n",
    "secfind.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Исполните действие на преобразование\n",
    "secfind.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что преобразования не будут отображать вывод и не будут выполняться, пока не будет вызвано действие. В следующей лекции: Advanced Spark и Python мы начнем видеть больше примеров такого преобразования и действие отношения!\n",
    "\n",
    "# Прекрасная работа!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
